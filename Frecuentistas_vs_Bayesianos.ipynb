{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSBkcJIgWWwT6TBQcf2hoj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaricelaMH/SIMULACION-II/blob/main/Frecuentistas_vs_Bayesianos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # F R E C U E N T I S T A S   _  V S  _  B A Y E S I A N O S\n",
        "\n",
        " Principalmente el desacuerdo de los frecuentistas con los bayesianos es la definición de probabilidad.\n",
        "\n",
        "1. **Definición frecuentista de la probabilidad:** Sostiene que la\n",
        "probabilidad de un evento es el límite al que tiende la frecuencia relativa de ocurrencia de dicho evento cuando el número de ensayos se incrementa indefinidamente. Es decir, según los frecuentistas, la probabilidad se basa en la observación empírica de eventos repetidos bajo condiciones similares.\n",
        "\n",
        "* Si realizamos un experimento muchas veces, la probabilidad de un evento $A$ se define como el valor al que tiende la proporción de veces que ocurre $A$ en relación al número total de ensayos.\n",
        "\n",
        "* Matemáticamente, se expresa como:\n",
        "\n",
        "\\\\\n",
        " $$ P(A) = \\lim_{n \\to ∞ } ( \\frac{\\text{Número de veces que ocurre el evento $A$}}{\\text{Número total de repeticiones del experimento}} )$$\n",
        "\n",
        "\\\\\n",
        "2. **Definición bayesiana de la probabilidad:** La probabilidad en el enfoque bayesiano se basa en el teorema de Bayes, que establece cómo actualizar las creencias o probabilidades iniciales (llamadas probabilidades a priori) cuando se adquieren nuevos datos o evidencia, resultando en una nueva probabilidad a posteriori.\n",
        "\n",
        "* El teorema de Bayes se expresa matemáticamente como:\n",
        "\n",
        "$$P(A|B)=\\frac{P(B|A)* P(A)}{P(B)}$$\n",
        "\n",
        "Donde:\n",
        "\n",
        "* $P(A∣B)$ es la probabilidad de $A$ dado que se ha observado $B$ (la probabilidad a posteriori).\n",
        "\n",
        "* $P(B∣A)$ es la probabilidad de observar $B$ si $A$ es verdadero.\n",
        "\n",
        "* $P(A)$ es la probabilidad de $A$ antes de observar $B$ (la probabilidad a priori)\n",
        "\n",
        "* $P(B)$ es la probabilidad total de observar $B$.\n",
        "\n",
        "Esta diferencia que podría decirse que es sutil, puede llevar, en la práctica, a enfoques muy diferentes para el análisis estadístico de datos. A continuación, exploraremos algunos ejemplos elegidos para ilustrar las diferencias de enfoque, junto con el código Python asociado para demostrar los aspectos prácticos de los enfoques frecuentista y bayesiano.\n",
        "\n"
      ],
      "metadata": {
        "id": "xK4-0a6b3XlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo 1: Mediciones del flujo de fotones\n",
        "Apuntamos un telescopio al cielo y observamos la luz que proviene de una sola estrella. Para simplificar, supondremos que el flujo fotónico real de la estrella es constante con el tiempo, es decir ,que tiene un valor fijo $F$, también ignoraremos efectos como los errores sistemáticos del fondo del cielo. Supondremos que se realizan una serie de $N$ mediciones, donde la i-ésima medición informa el flujo observado $F_i$ y el error $e_i$. La pregunta es dado este conjunto de mediciones $D = \\{F_i, e_i\\}$, ¿cuál es nuestra mejor estimación del flujo verdadero $F$?"
      ],
      "metadata": {
        "id": "GCJNv6cC-E0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* El articulo nos menciona primeramente que debemos de generar una muestra com media 1000 y en error definido como $e$:"
      ],
      "metadata": {
        "id": "-MpwcNpSkjNP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R-E_BG0_3Wiz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(2) # Si ejecutamos el código con la misma semilla (en este caso, 2), obtendremos los mismos números aleatorios\n",
        "\n",
        "# Generamos muestras de números aleatorios de una distribución normal\n",
        "e = np.random.normal(30, 3, 50) # Ponemos los datos de la media, desviacion estádar y tamaño de la muestra\n",
        "F = np.random.normal(1000, e) # Generamos un array de 50 valores con media 1000 y desviación estándar e"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Ahora dadas las mediciones y errores anteriores , ¿cuál es nuestra mejor estimación puntual del flujo verdadero?, por lo que el articulo nos da dos enfoques de solución, el frecuentista y el bayesiano."
      ],
      "metadata": {
        "id": "xPzBYB6umkM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### -> Enfoque frecuentista para la medición del flujo\n",
        "Por razones de simplicidad analítica y precisión numérica, la log-verosimilitud se utiliza para este fin, ya que maximizar la log-verosimilitud es equivalente a maximizar la verosimilitud, por lo que es más conveniente considerar la log-verosimilitud (transforma multiplicaciones de probabilidades en sumas) en lugar de la verosimilitud directa, por lo que esta biene definida como:\n",
        "\n",
        " $$ log \\mathscr{L} (D|F) = -\\frac{1}{2} \\sum_{i=1}^{N} [ log(2\\pi e_i^2)+\\frac{(F_i - F)^2}{e_i^2}]$$\n",
        "\n",
        "Nos gustaría determinar el valor de $F$ que maximiza la probabilidad. Para este problema simple, la maximización se puede calcular analíticamente, estableciendo (por ejemplo, estableciendo $\\frac{d log \\mathscr{L}}{dF}=0$), el resultado de esta maximización lleva a la siguiente estimación puntual de $F$:\n",
        "\n",
        "$$\\hat{F} = \\frac{\\sum{w_i F_i}}{\\sum{w_i}}$$\n",
        "\n",
        "Se toma cada valor en $e$ (la desviación estándar de cada elemento en $F$) y se calcula el peso correspondiente como\n",
        "\n",
        "$$w_i = \\frac{1}{e^2_i}$$\n",
        "\n",
        "el uso de $e^2_i$ indica que los valores con una mayor desviación estándar ($e$) tendrán menor peso en el promedio ponderado.\n",
        "\n",
        "De ogual manera podemos preguntarnos cuál es la incertidumbre de nuestra estimación. Una forma de lograrlo en el enfoque frecuentista es construir una aproximación gaussiana a la probabilidad máxima, por lo que se obtiene que:\n",
        "\n",
        "$$\\sigma_{\\hat{F}}=(\\sum_{i=1}^{N} w_i)^{-\\frac{1}{2}}$$\n",
        "\n",
        "lo implementamos de la siguiente manera:"
      ],
      "metadata": {
        "id": "YHvtRv-7piHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculamos los pesos inversamente proporcionales al cuadrado del error\n",
        "w = 1. / e ** 2  # 1./ es una forma de dividir 1.0 por el valor que sigue\n",
        "F_hat = np.sum(w * F) / np.sum(w)\n",
        "sigma_F = w.sum() ** -0.5\n",
        "print(\"El valor F que maximiza la probabilidad es: \",round(F_hat,2))\n",
        "print(\"La incertidumbre asociada a la estimación es: \",round(sigma_F,2))"
      ],
      "metadata": {
        "id": "u_9hB7LPpsRL",
        "outputId": "e9fd0486-d80a-437c-c831-ea8cf4361ddf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El valor F que maximiza la probabilidad es:  998.65\n",
            "La incertidumbre asociada a la estimación es:  4.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### -> Enfoque bayesiano para la medición del flujo\n",
        "\n",
        "Como es de esperar, el enfoque bayesiano comienza y termina con probabilidades. El resultado fundamental de interés es nuestro conocimiento de los parámetros en cuestión, para calcular este resultado, aplicamos a continuación el teorema de Bayes, una ley fundamental de la probabilidad:\n",
        "\n",
        "$$P(F|D)=\\frac{P(D|F) P(F)}{P(D)}$$\n",
        "\n",
        "donde:\n",
        "\n",
        "* $P(F|D)$: es la probabilidad de los parámetros del modelo dados los datos.\n",
        "* $P(D|F)$: que es proporcional a la $\\mathscr{L} (D|F)$ utilizada en el enfoque frecuentista.\n",
        "* $P(F)$: El modelo anterior, que codifica lo que sabíamos sobre el modelo antes de considerar los datos $D$.\n",
        "* $P(D)$: La evidencia del modelo, que en la práctica equivale simplemente a un término de normalización.\n",
        "\n",
        "su planteamiento es fundamentalmente contrario a la filosofia frecuentista, que dice que las probabilidades no tienen significado para parámetros fijos del modelo como F. Sin embargo, en la concepción bayesiana de la probabilidad esto no plantea ningún problema.\n",
        "Es decir, con una distribución previa plana en F, la distribución posterior bayesiana se maximiza exactamente en el mismo valor que el resultado frecuentista. Por lo tanto, a pesar de las diferencias filosóficas, vemos que las estimaciones puntuales bayesianas y frecuentistas son equivalentes para este problema simple."
      ],
      "metadata": {
        "id": "sJbCYELT76Gz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### -> Divergencia de resultados\n",
        "\n",
        "En el ejemplo simple anterior, los enfoques frecuentista y bayesiano arrojan básicamente el mismo resultado. Sin embargo, si bien es fácil demostrar que los dos enfoques suelen ser equivalentes para problemas simples, también es cierto que pueden divergir en gran medida en otras situaciones, esta divergencia suele manifestarse de dos maneras diferentes:\n",
        "\n",
        "1. El manejo de parámetros molestos: es decir, parámetros que afectan el resultado final, pero que no son de interés en ningún otro sentido.\n",
        "\n",
        "Un parámetro molesto es cualquier cantidad cuyo valor no es directamente relevante para el objetivo de un análisis, pero que sin embargo, es necesario para determinar el resultado que interesa.\n",
        "2. El diferente manejo de la incertidumbre: por ejemplo, la diferencia sutil (y a menudo pasada por alto) entre los intervalos de confianza frecuentistas y las regiones creíbles bayesianas.\n",
        "\n",
        "A continuación discutiremos ejemplos de esto."
      ],
      "metadata": {
        "id": "WpFPj3duABKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo 2: El juego de billar de Bayes\n",
        "\n",
        "Se trata de un juego de apuestas en el que Alicia y Bob apuestan sobre el resultado de un proceso que no pueden observar directamente.\n",
        "Alice y Bob entran en una habitación, detrás de una cortina hay una mesa de billar, que no pueden ver, su amiga Carol hace rodar una bola por la mesa y marca dónde cae. Una vez que la marca está en su lugar, Carol comienza a hacer rodar nuevas bolas por la mesa. Si la bola cae a la izquierda de la marca, Alice obtiene un punto; si cae a la derecha de la marca, Bob obtiene un punto. Podemos suponer que los lanzamientos de Carol son imparciales: es decir, las bolas tienen la misma probabilidad de terminar en cualquier lugar de la mesa. La primera persona que alcance los seis puntos gana el juego, en este caso, la ubicación de la marca (determinada por el primer lanzamiento) puede considerarse un parámetro molesto: es desconocido y no tiene interés inmediato, pero claramente debe tenerse en cuenta al predecir el resultado de los lanzamientos posteriores. Si el primer lanzamiento se establece muy a la derecha, los lanzamientos posteriores favorecerán a Alice. Si se establece muy a la izquierda, Bob será el favorecido. Con esta configuración, buscamos responder a esta pregunta: en un juego en particular, después de ocho lanzamientos, Alice tiene cinco puntos y Bob tiene tres puntos. ¿Cuál es la probabilidad de que Bob obtenga seis puntos y gane el juego?. Intuitivamente, nos damos cuenta de que, como Alice recibió cinco de los ocho puntos, la ubicación del marcador probablemente la favorezca. Dado que tiene tres oportunidades de obtener un sexto punto antes de que Bob pueda ganar, parece que lo ha conseguido. Pero, cuantitativamente hablando, ¿cuál es la probabilidad de que Bob persista para ganar?"
      ],
      "metadata": {
        "id": "Yjog7T1VDW8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### -> Un enfoque frecuentista ingenuo\n",
        "\n",
        "Como cinco bolas de ocho calleron en el lado del marcador de Alicia, calculamos la estimación de máxima verosimilitud de p, dada por:\n",
        "\n",
        "$$\\hat{p} = \\frac{5}{8}$$\n",
        "\n",
        "De la probabilidad binomial se sigue un resultado de manera directa. Suponiendo esta probabilidad máxima, podemos calcular la probabilidad de que Bob gane, lo que requiere que obtenga un punto en cada uno de los tres lanzamientos siguientes. Esto viene dado por:\n",
        "\n",
        "$$P(B)=(1-\\hat{p})^3$$\n",
        "\n",
        "lo implementamos de la siguiente manera"
      ],
      "metadata": {
        "id": "4Z-6E2okFuXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p_hat = 5/8\n",
        "P_B = (1-p_hat)**3\n",
        "print(\"La probabilidad de que Bob gane es: \",round(P_B,2))"
      ],
      "metadata": {
        "id": "auiZ5pLXGwF2",
        "outputId": "7d2ee156-ae75-4eb8-f417-cc805841079f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La probabilidad de que Bob gane es:  0.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### -> Enfoque bayesiano\n",
        "Un enfoque bayesiano para este problema implica marginalizar (es decir, integrar) la $p$ desconocida de modo que, suponiendo que lo anterior sea preciso, nuestro resultado debe ser independiente de su valor real.\n",
        "En este sentido, consideraremos las siguientes cantidades:\n",
        "1. $B=$ Bob gana.\n",
        "2. $D=$ Datos observados, es decir $D=(5,3)$\n",
        "3. $\\rho =$ probabilidad desconocida de que una pelota caiga en el lado de Alicia durante el juego actual.\n",
        "Comenzamos aplicando la definición de probabilidad condicional para\n",
        "expandir el término $P(B,\\rho|D)$:\n",
        "\n",
        "$$P(B|D)=\\int P(B|\\rho,D)P(\\rho|D)d\\rho$$\n",
        "\n",
        "Finalmente, utilizando la misma identidad de probabilidad con la que comenzamos, podemos expandir $P(D)$ en el denominador para encontrar:\n",
        "\n",
        "$$P(B|D)=\\frac{\\int P(B|\\rho,D)P(D|\\rho)P(\\rho)d\\rho}{\\int P(D|\\rho)P(\\rho)d\\rho}$$\n",
        "\n",
        "donde:\n",
        "\n",
        "* $P(B∣\\rho,D):$ Es la probabilidad de que Bob gane tres veces seguidas dados un valor de $\\rho$ (probabilidad de ganar un juego) y los datos $D$. Se calcula como $(1-\\rho)^3$\n",
        "* $P(D|\\rho):$ Es la probabilidad de obtener exactamente 5 éxitos (victorias de Alice) en 8 intentos, dado el valor $\\rho$. Esto se obtiene usando la distribución binomial:\n",
        "$P(D|\\rho)$ -> $\\rho^5 (1-\\rho)^3$\n",
        "* $P(\\rho):$ Es la probabilidad a priori de $\\rho$. En este caso, se asume que $\\rho$ es uniforme entre 0 y 1.\n",
        "\n",
        "por lo que\n",
        "\n",
        "$$P(B|D)=\\frac{\\int_{0}^{1} (1-\\rho)^6 \\rho^5 d\\rho}{\\int_{0}^{1} (1-\\rho)^3 \\rho^5 d\\rho}$$\n",
        "\n",
        "Estas integrales son instancias de la función beta, por lo que podemos\n",
        "evaluar rápidamente el resultado usando scipy:\n"
      ],
      "metadata": {
        "id": "WtyZbZzLG6Cr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import beta # Se importa la función beta\n",
        "P_B_D = beta(6+1, 5+1) / beta(3+1, 5+1) # los valores pueden representar el número de éxitos y fracasos en un contexto de estimación bayesiana\n",
        "print(\"La probabilidad de que Bob gane es: \",round(P_B_D,2))"
      ],
      "metadata": {
        "id": "bxRhhHFNOcvj",
        "outputId": "26c2895d-6893-401c-cc4e-f3532da701f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La probabilidad de que Bob gane es:  0.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* El enfoque bayesiano da probabilidades de 10 a 1 en contra de Bob, mientras que el enfoque frecuentista ingenuo da probabilidades de 18 a 1 en contra de Bob. Entonces, ¿cuál es correcto?\n",
        "Para un problema tan simple como este, podemos responder a esta pregunta empíricamente simulando una gran cantidad de juegos y contando la fracción de juegos adecuados que Bob gana. El resultado de dicha simulación confirma el resultado\n",
        "bayesiano: 10 a 1 en contra de la victoria de Bob. ¿El frecuentismo es incorrecto?, no necesariamente: en este caso, el resultado incorrecto es más una cuestión de que el enfoque sea \"ingenuo\" que de que sea\n",
        "\"frecuentista\"."
      ],
      "metadata": {
        "id": "TbqPi5q0PNFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confianza vs. Credibilidad\n",
        "\n",
        "A la hora de construir un límite estándar del 95% sobre un parámetro $\\theta$:\n",
        " * Un bavesiano diría: \"Dados nuestros datos observados, hay un 95% de probabilidad de que el verdadero valor de $\\theta$ se encuentre dentro de la región creíble\".\n",
        " * Un frecuentista diría: \"Si este experimento se repite muchas veces, en el 95% de estos casos el intervalo de confianza calculado contendrá el verdadero 0.25\".\n",
        "\n",
        "Observemos la sutil diferencia: el bayesiano enuncia una probabilidad sobre el valor del parámetro dada una región creíble fija. El frecuentista enuncia una probabilidad sobre el intervalo de confianza en sí dado un valor de parámetro fijo. Esta distinción se desprende directamente de la definición de probabilidad que se ha analizado anteriormente, la probabilidad bayesiana es una afirmación del grado de conocimiento sobre un parámetro, la probabilidad frecuentista es una afirmación de la frecuencia límite a largo plazo de las cantidades (como el intervalo de confianza) derivadas de los datos.\n",
        "\n",
        "El frecuentismo no busca una afirmación probabilística sobre un intervalo fijo, como lo hace el enfoque bayesiano; en cambio, busca afirmaciones probabilísticas sobre un conjunto de intervalos construidos, siendo el intervalo calculado particular sólo una única extracción de entre ellos."
      ],
      "metadata": {
        "id": "ShqSwiNwRR0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo 3 (Bayesianismo): El método Monte Carlo con cadenas de Markov\n",
        "\n",
        "Un punto de inflexión en la computación bayesiana práctica fue el desarrollo y la aplicación de métodos de muestreo como Markov Chain Monte Carlo, los cuales son una clase de algoritmos que pueden caracterizar de manera eficiente incluso distribuciones posteriores de alta dimensión mediante la extracción de muestras\n",
        "aleatorias de manera que los puntos se distribuyan de acuerdo hacia la parte posterior.\n",
        "A continuación, propondremos vemos modelo sencillo y compararemos un enfoque frecuentista estándar\n",
        "con tres implementaciones de MCMC disponibles en Python.\n",
        "\n"
      ],
      "metadata": {
        "id": "3hTtL8FWTs5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### -> Un modelo lineal simple\n",
        "Consideremos un modelo lineal simple de tres parámetros que ajusta una línea recta a datos con errores desconocidos. Los parámetros serán la intersección con el eje $\\alpha$, la pendiente $\\beta$ y la dispersión normal (desconocida) o sobre la línea.\n",
        "Para los datos $D = {(xi, yi)}$, el modelo es\n",
        "\n",
        "$$ \\hat{y}(x_i|\\alpha,\\beta)=\\alpha + \\beta x_i$$\n",
        "\n",
        "y la probabilidad es el producto de la distribución gaussiana para cada\n",
        "punto:\n",
        " $$\\mathscr{L}(D|\\alpha,\\beta,\\sigma)=(2\\pi\\sigma^2)^{-\\frac{N}{2}} \\prod_{i=1}^{N} \\text{ exp }[\\frac{-{[y_i-\\hat{y}(x_i|\\alpha,\\beta)]^2}}{2\\sigma^2}] $$\n",
        "\n",
        " Evaluaremos este modelo en el siguiente conjunto de datos:\n"
      ],
      "metadata": {
        "id": "7Fog8YSKUyfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42) # Fija la semilla del generador de números aleatorios para asegurar la reproducibilidad de los resultados\n",
        "theta_true = (25, 0.5) # Define los parámetros del modelo lineal (intersección y pendiente)\n",
        "xdata = 100 * np.random.random(20) # Genera 20 valores aleatorios de x entre 0 y 100\n",
        "ydata = theta_true[0] + theta_true[1] * xdata # Calcula los valores de y correspondientes a la línea recta\n",
        "ydata = np.random.normal(ydata, 10) # # Agrega ruido aleatorio a los valores de y con una desviación estándar de 10\n"
      ],
      "metadata": {
        "id": "CPsQKXJMRSiw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####-> Solución frecuentista\n",
        "\n",
        "Se puede encontrar una solución frecuentista calculando la estimación puntual de máxima verosimilitud.\n",
        "Si definimos el vector $\\theta = [\\alpha \\beta]^T$ el vector de respuesta, $Y=[y_1 , y_2, y_3 , ... , y_N]^T$ y la matriz de diseño:\n",
        "\n",
        "$$\\begin{equation}\n",
        "\\begin{bmatrix}\n",
        " 1 & 1 & 1 & ... & 1\\\\\n",
        "x_1 & x_2 & x_3 & ... & x_N\n",
        "\\end{bmatrix}\n",
        "\\end{equation}$$\n",
        "\n",
        "Se puede demostrar que la solución de máxima verosimilitud\n",
        "\n",
        "$$\\hat{\\theta}=(X^T X )^{-1} (X^T Y) $$"
      ],
      "metadata": {
        "id": "UNdeMb1cS4KK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.vstack([np.ones_like(xdata), xdata]).T #Creamos la matriz X de diseño para el modelo de regresión lineal\n",
        "theta_hat = np.linalg.solve(np.dot(X.T, X),np.dot(X.T, ydata)) #np.dont(..) calcula el producto de matrices\n",
        "                                                               # np.linalg.solve(...) resuelve el sistema de ecuaciones para obtener theta\n",
        "y_hat = np.dot(X, theta_hat) # Se predice el valor de y tomando en cuenta los coeficientres obtenidos por theta\n",
        "sigma_hat = np.std(ydata - y_hat) # Se calcula la desviación estándar de los residuos (errores)\n",
        "Sigma = sigma_hat ** 2 *  np.linalg.inv(np.dot(X.T, X)) # Se calcula la matriz de covarianza de los estimadores de los coeficientes theta"
      ],
      "metadata": {
        "id": "6DBIG8D6FgS9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la práctica, el enfoque frecuentista suele depender de muchos más diagnósticos estadísticos que la máxima verosimilitud y el intervalo de confianza. Para este problema, se puede utilizar de la siguiente manera:"
      ],
      "metadata": {
        "id": "A35m9N0f4xWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm # version 0.5\n",
        "X = sm.add_constant(xdata) # sm.add_constant(...) agrega un término constante al conjunto de datos de entrada xdata\n",
        "result = sm.OLS(ydata, X).fit() # sm.OLS(...) crea un modelo de regresión lineal utilizando el metodo de minimos cuadrados\n",
        "                                # .fit(...) ajusta el modelo a los datos, estimando los coeficientes de la regresión\n",
        "sigma_hat = result.params    # Contiene los coeficientes estimados (o parámetros) del modelo\n",
        "Sigma = result.cov_params()   # Contiene la matriz de covarianza de los coeficientes estimados\n",
        "print(result.summary2())  # Imprime un resumen detallado de los resultados del ajuste del modelo de regresión"
      ],
      "metadata": {
        "id": "nPtvbfJ85FRf",
        "outputId": "51022646-3bf7-49e7-b06e-bf8d62539269",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Results: Ordinary least squares\n",
            "=================================================================\n",
            "Model:              OLS              Adj. R-squared:     0.683   \n",
            "Dependent Variable: y                AIC:                147.7737\n",
            "Date:               2024-10-12 02:47 BIC:                149.7651\n",
            "No. Observations:   20               Log-Likelihood:     -71.887 \n",
            "Df Model:           1                F-statistic:        41.97   \n",
            "Df Residuals:       18               Prob (F-statistic): 4.30e-06\n",
            "R-squared:          0.700            Scale:              86.157  \n",
            "-------------------------------------------------------------------\n",
            "            Coef.    Std.Err.     t      P>|t|     [0.025    0.975]\n",
            "-------------------------------------------------------------------\n",
            "const      24.6361     3.7871   6.5053   0.0000   16.6797   32.5924\n",
            "x1          0.4483     0.0692   6.4782   0.0000    0.3029    0.5937\n",
            "-----------------------------------------------------------------\n",
            "Omnibus:              1.996        Durbin-Watson:           2.758\n",
            "Prob(Omnibus):        0.369        Jarque-Bera (JB):        1.634\n",
            "Skew:                 0.651        Prob(JB):                0.442\n",
            "Kurtosis:             2.486        Condition No.:           100  \n",
            "=================================================================\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the\n",
            "errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podríamos imaginarnos cómo abordar este problema transformando las varables, por ejemplo, haciendo uso del Prior de Jeffreys en el contexto de un modelo de regresión lineal. Se propone transformar variables utilizando un prior plano en el ángulo que la línea forma con el eje $x$, en lugar de usar la pendiente. A través de transformaciones y usando el Jacobiano, se deduce que la probabilidad conjunta de los parámetros $\\alpha$ y $\\beta$ debe ser proporcional a $$(1+\\beta^2)^{\\frac{3}{2}}$$\n",
        "\n",
        "por lo que se argumenta que para el parámetro $\\sigma$ (la desviación estándar), el Prior de Jeffreys es inversamente proporcional a $\\sigma$, es decir, $$P(\\sigma)∝1/\\sigma$$, lo que equivale a un prior plano en el logaritmo de $\\sigma$,por lo que el prior no informativo para el problema de regresión lineal es proporcional a $$\\frac{1}{\\sigma}(1+\\beta^2)^{-\\frac{3}{2}}$$ y se puede usar este prior junto con la verosimilitud para evaluar el posterior usando métodos como MCMC.\n",
        "\n"
      ],
      "metadata": {
        "id": "CB31fFJD6XlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solición con presentador\n",
        "\n",
        "El paquete emcee en Python, implementa un método de muestreo MCMC llamado Affine Invariant Ensemble MCMC. Este método es una versión avanzada de MCMC. El paquete emcee es ligero y fácil de usar, ya que solo requiere definir una función en Python que represente el logaritmo del posterior.\n",
        "\n",
        "Para mayor claridad, la definición del posterior se descompone en dos partes: el log-prior (logaritmo del prior) y el log-likelihood (logaritmo de la verosimilitud), que se utilizan para el muestreo."
      ],
      "metadata": {
        "id": "RqQSiLxR9dlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q emcee\n",
        "\n",
        "import emcee # librería para realizar muestreo de cadenas de Markov\n",
        "import numpy as np # Make sure to import numpy\n",
        "\n",
        "\n",
        "def log_prior(theta):\n",
        "    alpha, beta, sigma = theta\n",
        "    if sigma < 0:\n",
        "        return -np.inf\n",
        "    else:\n",
        "        return (-1.5 * np.log(1 + beta ** 2) - np.log(sigma))\n",
        "\n",
        "def log_like(theta, x, y):  # Funcion de verosimilitud\n",
        "    alpha, beta, sigma = theta\n",
        "    y_model = alpha + beta * x\n",
        "    # Fix: Use np.pi * sigma instead of np.pisigma\n",
        "    return -0.5 * np.sum(np.log(2 * np.pi * sigma ** 2) + (y - y_model) ** 2 / sigma ** 2)\n",
        "\n",
        "def log_posterior(theta, x, y):\n",
        "    return log_prior(theta) + log_like(theta, x, y)\n",
        "\n"
      ],
      "metadata": {
        "id": "9eZWZg359w98",
        "outputId": "d9d265b2-4790-4f3e-ff84-a245916dc9f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/47.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, configuramos el cálculo. emcee combina varios \"caminantes\" que interactúan, cada uno de los cuales genera su propia cadena de Markov. También especificaremos un período de rodaje para permitir\n",
        "que las cadenas se estabilicen antes de dibujar los trazos finales:"
      ],
      "metadata": {
        "id": "g7RdknphActv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ndim=3 # Indica que se muestrearán tres parámetros del modelo\n",
        "nwalkers=50 # Utiliza 50 caminantes para explorar el espacio de parámetros.\n",
        "nburn=100 # Descartará los primeros 100 pasos de cada caminante\n",
        "nsteps=20 # Indica que los caminantes realizarán 20 pasos después del \"burn-in\"\n",
        "starting_guesses=np.random.rand(nwalkers,ndim) #Los caminantes comienzan en posiciones aleatorias en un cubo tridimensional de valores entre 0 y 1."
      ],
      "metadata": {
        "id": "s_iaRzXf-Z8z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampler=emcee.EnsembleSampler(nwalkers,ndim,log_posterior,args=[xdata,ydata])\n",
        "sampler.run_mcmc(starting_guesses,nsteps) # se ejecuta el muestreo\n",
        "# chain is of shape (nwalkers, nsteps, ndim):\n",
        "# discard burn-in points and reshape:\n",
        "trace = sampler.chain[:, nburn:, :]\n",
        "trace = trace.reshape(-1, ndim).T"
      ],
      "metadata": {
        "id": "baF_VmTRBVPe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuacion se muestra un paquete que utiliza el clásico muestreador Metropolis-Hastings y ofrece muchas características integradas, como soporte para el muestreo eficiente de distribuciones previas comunes. Debido a estas capacidades, PyMC requiere un poco más de configuración inicial en comparación con emcee, pero resulta ser una herramienta muy poderosa para realizar inferencias bayesianas flexibles."
      ],
      "metadata": {
        "id": "Q0Z5PoyuC9Dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pymc==4.0\n"
      ],
      "metadata": {
        "id": "iRFccJhJeO23",
        "outputId": "0016325c-85b7-42de-e8e4-493a3da6aa4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m554.5/554.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for aeppl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for aesara (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymc # version 2.3\n",
        "\n",
        "alpha = pymc.Uniform(alpha, -100, 100)\n",
        "@pymc.stochastic(observed=False)\n",
        "\n",
        "def beta(value=0):\n",
        "    return -1.5 * np.log(1 + value**2)\n",
        "\n",
        "@pymc.stochastic(observed=False)\n",
        "\n",
        "def sigma(value=1):\n",
        "    return -np.log(abs(value))\n",
        "\n",
        "# Define the form of the model and likelihood\n",
        "@pymc.deterministic\n",
        "\n",
        "def y_model(x=xdata, alpha=alpha, beta=beta):\n",
        "\n",
        "   return alpha + beta * x\n",
        "\n",
        "y = pymc.Normal(y, mu=y_model, tau=1./sigma**2,\n",
        "observed=True, value=ydata)\n",
        "# package the full model in a dictionary\n",
        "model = dict(alpha=alpha, beta=beta, sigma=sigma,y_model=y_model, y=y)\n",
        "\n"
      ],
      "metadata": {
        "id": "AIyrBP8EIfCr",
        "outputId": "2bd4c023-36b0-4a61-d621-2daab27d3db1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "\nCould not import 'mkl'.  If you are using conda, update the numpy\npackages to the latest build otherwise, set MKL_THREADING_LAYER=GNU in\nyour environment for MKL 2018.\n\nIf you have MKL 2017 install and are not in a conda environment you\ncan set the Aesara flag blas__check_openmp to False.  Be warned that if\nyou set this flag and don't set the appropriate environment or make\nsure you have the right version you *will* get wrong results.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNoSectionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/aesara/configparser.py\u001b[0m in \u001b[0;36mfetch_val_for_key\u001b[0;34m(self, key, delete_key)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aesara_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterpolationError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/configparser.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, section, option, raw, vars, fallback)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m             \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unify_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mNoSectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/configparser.py\u001b[0m in \u001b[0;36m_unify_values\u001b[0;34m(self, section, vars)\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msection\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_section\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mNoSectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m         \u001b[0;31m# Update with the entry specific variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNoSectionError\u001b[0m: No section: 'blas'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/aesara/configparser.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, cls, type_, delete_key)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                 \u001b[0mval_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_val_for_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelete_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelete_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_default\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/aesara/configparser.py\u001b[0m in \u001b[0;36mfetch_val_for_key\u001b[0;34m(self, key, delete_key)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNoOptionError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoSectionError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'blas__ldflags'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/aesara/link/c/cmodule.py\u001b[0m in \u001b[0;36mcheck_mkl_openmp\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2663\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2664\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mmkl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mkl'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-885d90b98fbb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpymc\u001b[0m \u001b[0;31m# version 2.3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpymc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mpymc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochastic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pymc/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0m__set_compiler_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpymc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pymc/__init__.py\u001b[0m in \u001b[0;36m__set_compiler_flags\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__set_compiler_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Workarounds for Aesara compiler problems on various platforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0maesara\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maesara\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcc__cxxflags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/aesara/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;31m# isort: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0maesara\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscalar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m from aesara.compile import (\n\u001b[1;32m    128\u001b[0m     \u001b[0mIn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/aesara/tensor/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# adds shared-variable constructors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0maesara\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msharedvar\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m from aesara.tensor import (  # noqa\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mbasic_opt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mblas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/aesara/tensor/blas.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0maesara\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbasic\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0maesara\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_opt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlocal_dimshuffle_lift\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0maesara\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblas_headers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mblas_header_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblas_header_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0maesara\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melemwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDimShuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mElemwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0maesara\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNotScalarConstantError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/aesara/tensor/blas_headers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblas__ldflags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Using NumPy C-API based implementation for BLAS functions.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/aesara/configparser.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, cls, type_, delete_key)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m                     \u001b[0mval_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m                     \u001b[0mval_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/aesara/link/c/cmodule.py\u001b[0m in \u001b[0;36mdefault_blas_ldflags\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2870\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2871\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"mkl\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2872\u001b[0;31m                 \u001b[0mcheck_mkl_openmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/aesara/link/c/cmodule.py\u001b[0m in \u001b[0;36mcheck_mkl_openmp\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2673\u001b[0m             )\n\u001b[1;32m   2674\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m   2676\u001b[0m             \"\"\"\n\u001b[1;32m   2677\u001b[0m \u001b[0mCould\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m'mkl'\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mIf\u001b[0m \u001b[0myou\u001b[0m \u001b[0mare\u001b[0m \u001b[0musing\u001b[0m \u001b[0mconda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \nCould not import 'mkl'.  If you are using conda, update the numpy\npackages to the latest build otherwise, set MKL_THREADING_LAYER=GNU in\nyour environment for MKL 2018.\n\nIf you have MKL 2017 install and are not in a conda environment you\ncan set the Aesara flag blas__check_openmp to False.  Be warned that if\nyou set this flag and don't set the appropriate environment or make\nsure you have the right version you *will* get wrong results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "S=pymc.MCMC(model)\n",
        "S.sample(iter=10000,burn=1000)\n",
        "trace=[S.trace('alpha')[:],S.trace('beta')[:],S.trace('sigma')[:]]"
      ],
      "metadata": {
        "id": "sY8j2F7tKXdQ",
        "outputId": "a5df18b9-4d27-43a4-9a7d-bc19039e79be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pymc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-4c2da7da1225>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpymc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMCMC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mburn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'alpha'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'beta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sigma'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pymc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solución con PyStan\n",
        "El lenguaje Stan está diseñado especificamente para la expresión de modelos probabilísticos; PyStan permite que los modelos Stan especificados en forma de cadenas de Python sean analizados, compilados y ejecutados por la biblioteca Stan."
      ],
      "metadata": {
        "id": "E1qf3CTYKP2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pystan\n",
        "import pystan # version 2.2\n",
        "model_code = \"\"\"\n",
        "data {\n",
        "int<lower=0> N; // number of points\n",
        "real x[N]; // x values\n",
        "real y[N]; // y values\n",
        "}\n",
        "parameters {\n",
        "real alpha_perp;\n",
        "real<lower=-pi()/2, upper=pi()/2> theta;\n",
        "real log_sigma;\n",
        "}\n",
        "transformed parameters {\n",
        "real alpha;\n",
        "real beta;\n",
        "real sigma;\n",
        "real ymodel[N];\n",
        "alpha <- alpha_perp / cos(theta);\n",
        "beta <- sin(theta);\n",
        "sigma <- exp(log_sigma);\n",
        "for (j in 1:N)\n",
        "ymodel[j] <- alpha + beta * x[j];\n",
        "}\n",
        "model {\n",
        "y ~ normal(ymodel, sigma);\n",
        "}\n",
        "\"\"\"\n",
        "# perform the fit & extract traces\n",
        "data = {N: len(xdata), x: xdata, y: ydata}\n",
        "fit = pystan.stan(model_code=model_code, data=data,\n",
        "\n",
        "iter=25000, chains=4)\n",
        "\n",
        "tr = fit.extract()\n",
        "trace = [tr[alpha], tr[beta], tr[sigma]]\n"
      ],
      "metadata": {
        "id": "RhRoMKxjKXbg",
        "outputId": "20f4437e-dfab-45c3-ff00-4c552d0ed0fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pystan'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1f431d0b6b3a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -q pystan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpystan\u001b[0m \u001b[0;31m# version 2.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m model_code = \"\"\"\n\u001b[1;32m      4\u001b[0m data {\n\u001b[1;32m      5\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pystan'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aunque no se logro copilar los códigos que nos mostraba el articulo con los diferentes tipos de soluciones debido a las versiones de las paqueterias que se manejan actualmente, el articulo nos muestra una imagen en la cual se ve el comportamiento de las elipses, por lo que combinando estas ideas y por varios ejemplos realizados anteriormente sobre el uso del frecuentismo y el bayesianismo en un problema de regresión lineal más realista, entonces nos preguntamos ¿Cuál es entonces el mejor enfoque?, esta es una cuestión de ideología personal, pero también depende de la naturaleza del problema en cuestión. Los enfoques frecuentistas suelen calcularse fácilmente y son adecuados para procesos y mediciones verdaderamente repetibles, pero pueden encontrarse con obstáculos con conjuntos pequeños de datos y modelos que se alejan mucho del modelo gaussiano. Existen herramientas frecuentistas para estas situaciones, pero a menudo requieren consideraciones sutiles y conocimientos especializados. Los enfoques bayesianos requieren la especificación de un valor previo potencialmente subjetivo y a menudo implican un cálculo intensivo mediante MCMC."
      ],
      "metadata": {
        "id": "Su-B0VCvMkLz"
      }
    }
  ]
}
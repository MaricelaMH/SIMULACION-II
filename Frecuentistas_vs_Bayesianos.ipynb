{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOltuT3OVxzAH++fgV7o/vx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaricelaMH/SIMULACION-II/blob/main/Frecuentistas_vs_Bayesianos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # F R E C U E N T I S T A S   _  V S  _  B A Y E S I A N O S\n",
        "\n",
        " Principalmente el desacuerdo de los frecuentistas con los bayesianos es la definición de probabilidad.\n",
        "\n",
        "1. **Definición frecuentista de la probabilidad:** Sostiene que la\n",
        "probabilidad de un evento es el límite al que tiende la frecuencia relativa de ocurrencia de dicho evento cuando el número de ensayos se incrementa indefinidamente. Es decir, según los frecuentistas, la probabilidad se basa en la observación empírica de eventos repetidos bajo condiciones similares.\n",
        "\n",
        "* Si realizamos un experimento muchas veces, la probabilidad de un evento $A$ se define como el valor al que tiende la proporción de veces que ocurre $A$ en relación al número total de ensayos.\n",
        "\n",
        "* Matemáticamente, se expresa como:\n",
        "\n",
        "\\\\\n",
        " $$ P(A) = \\lim_{n \\to ∞ } ( \\frac{\\text{Número de veces que ocurre el evento $A$}}{\\text{Número total de repeticiones del experimento}} )$$\n",
        "\n",
        "\\\\\n",
        "2. **Definición bayesiana de la probabilidad:** La probabilidad en el enfoque bayesiano se basa en el teorema de Bayes, que establece cómo actualizar las creencias o probabilidades iniciales (llamadas probabilidades a priori) cuando se adquieren nuevos datos o evidencia, resultando en una nueva probabilidad a posteriori.\n",
        "\n",
        "* El teorema de Bayes se expresa matemáticamente como:\n",
        "\n",
        "$$P(A|B)=\\frac{P(B|A)* P(A)}{P(B)}$$\n",
        "\n",
        "Donde:\n",
        "\n",
        "* $P(A∣B)$ es la probabilidad de $A$ dado que se ha observado $B$ (la probabilidad a posteriori).\n",
        "\n",
        "* $P(B∣A)$ es la probabilidad de observar $B$ si $A$ es verdadero.\n",
        "\n",
        "* $P(A)$ es la probabilidad de $A$ antes de observar $B$ (la probabilidad a priori)\n",
        "\n",
        "* $P(B)$ es la probabilidad total de observar $B$.\n",
        "\n",
        "Esta diferencia que podría decirse que es sutil, puede llevar, en la práctica, a enfoques muy diferentes para el análisis estadístico de datos. A continuación, exploraremos algunos ejemplos elegidos para ilustrar las diferencias de enfoque, junto con el código Python asociado para demostrar los aspectos prácticos de los enfoques frecuentista y bayesiano.\n",
        "\n"
      ],
      "metadata": {
        "id": "xK4-0a6b3XlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo 1: Mediciones del flujo de fotones\n",
        "Apuntamos un telescopio al cielo y observamos la luz que proviene de una sola estrella. Para simplificar, supondremos que el flujo fotónico real de la estrella es constante con el tiempo, es decir ,que tiene un valor fijo $F$, también ignoraremos efectos como los errores sistemáticos del fondo del cielo. Supondremos que se realizan una serie de $N$ mediciones, donde la i-ésima medición informa el flujo observado $F_i$ y el error $e_i$. La pregunta es dado este conjunto de mediciones $D = \\{F_i, e_i\\}$, ¿cuál es nuestra mejor estimación del flujo verdadero $F$?"
      ],
      "metadata": {
        "id": "GCJNv6cC-E0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* El articulo nos menciona primeramente que debemos de generar una muestra com media 1000 y en error definido como $e$:"
      ],
      "metadata": {
        "id": "-MpwcNpSkjNP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "R-E_BG0_3Wiz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(2) # Si ejecutamos el código con la misma semilla (en este caso, 2), obtendremos los mismos números aleatorios\n",
        "\n",
        "# Generamos muestras de números aleatorios de una distribución normal\n",
        "e = np.random.normal(30, 3, 50) # Ponemos los datos de la media, desviacion estádar y tamaño de la muestra\n",
        "F = np.random.normal(1000, e) # Generamos un array de 50 valores con media 1000 y desviación estándar e"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Ahora dadas las mediciones y errores anteriores , ¿cuál es nuestra mejor estimación puntual del flujo verdadero?, por lo que el articulo nos da dos enfoques de solución, el frecuentista y el bayesiano."
      ],
      "metadata": {
        "id": "xPzBYB6umkM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### -> Enfoque frecuentista para la medición del flujo\n",
        "Por razones de simplicidad analítica y precisión numérica, la log-verosimilitud se utiliza para este fin, ya que maximizar la log-verosimilitud es equivalente a maximizar la verosimilitud, por lo que es más conveniente considerar la log-verosimilitud (transforma multiplicaciones de probabilidades en sumas) en lugar de la verosimilitud directa, por lo que esta biene definida como:\n",
        "\n",
        " $$ log \\mathscr{L} (D|F) = -\\frac{1}{2} \\sum_{i=1}^{N} [ log(2\\pi e_i^2)+\\frac{(F_i - F)^2}{e_i^2}]$$\n",
        "\n",
        "Nos gustaría determinar el valor de $F$ que maximiza la probabilidad. Para este problema simple, la maximización se puede calcular analíticamente, estableciendo (por ejemplo, estableciendo $\\frac{d log \\mathscr{L}}{dF}=0$), el resultado de esta maximización lleva a la siguiente estimación puntual de $F$:\n",
        "\n",
        "$$\\hat{F} = \\frac{\\sum{w_i F_i}}{\\sum{w_i}}$$\n",
        "\n",
        "Se toma cada valor en $e$ (la desviación estándar de cada elemento en $F$) y se calcula el peso correspondiente como\n",
        "\n",
        "$$w_i = \\frac{1}{e^2_i}$$\n",
        "\n",
        "el uso de $e^2_i$ indica que los valores con una mayor desviación estándar ($e$) tendrán menor peso en el promedio ponderado.\n",
        "\n",
        "De ogual manera podemos preguntarnos cuál es la incertidumbre de nuestra estimación. Una forma de lograrlo en el enfoque frecuentista es construir una aproximación gaussiana a la probabilidad máxima, por lo que se obtiene que:\n",
        "\n",
        "$$\\sigma_{\\hat{F}}=(\\sum_{i=1}^{N} w_i)^{-\\frac{1}{2}}$$\n",
        "\n",
        "lo implementamos de la siguiente manera:"
      ],
      "metadata": {
        "id": "YHvtRv-7piHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculamos los pesos inversamente proporcionales al cuadrado del error\n",
        "w = 1. / e ** 2  # 1./ es una forma de dividir 1.0 por el valor que sigue\n",
        "F_hat = np.sum(w * F) / np.sum(w)\n",
        "sigma_F = w.sum() ** -0.5\n",
        "print(\"El valor F que maximiza la probabilidad es: \",round(F_hat,2))\n",
        "print(\"La incertidumbre asociada a la estimación es: \",round(sigma_F,2))"
      ],
      "metadata": {
        "id": "u_9hB7LPpsRL",
        "outputId": "73d5a89f-5ba9-481f-f9fc-272900ce56d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El valor F que maximiza la probabilidad es:  998.65\n",
            "La incertidumbre asociada a la estimación es:  4.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### -> Enfoque bayesiano para la medición del flujo\n",
        "\n",
        "Como es de esperar, el enfoque bayesiano comienza y termina con probabilidades. El resultado fundamental de interés es nuestro conocimiento de los parámetros en cuestión, para calcular este resultado, aplicamos a continuación el teorema de Bayes, una ley fundamental de la probabilidad:\n",
        "\n",
        "$$P(F|D)=\\frac{P(D|F) P(F)}{P(D)}$$\n",
        "\n",
        "donde:\n",
        "\n",
        "* $P(F|D)$: es la probabilidad de los parámetros del modelo dados los datos.\n",
        "* $P(D|F)$: que es proporcional a la $\\mathscr{L} (D|F)$ utilizada en el enfoque frecuentista.\n",
        "* $P(F)$: El modelo anterior, que codifica lo que sabíamos sobre el modelo antes de considerar los datos $D$.\n",
        "* $P(D)$: La evidencia del modelo, que en la práctica equivale simplemente a un término de normalización.\n",
        "\n",
        "su planteamiento es fundamentalmente contrario a la filosofia frecuentista, que dice que las probabilidades no tienen significado para parámetros fijos del modelo como F. Sin embargo, en la concepción bayesiana de la probabilidad esto no plantea ningún problema.\n",
        "Es decir, con una distribución previa plana en F, la distribución posterior bayesiana se maximiza exactamente en el mismo valor que el resultado frecuentista. Por lo tanto, a pesar de las diferencias filosóficas, vemos que las estimaciones puntuales bayesianas y frecuentistas son equivalentes para este problema simple."
      ],
      "metadata": {
        "id": "sJbCYELT76Gz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### -> Divergencia de resultados\n",
        "\n",
        "En el ejemplo simple anterior, los enfoques frecuentista y bayesiano arrojan básicamente el mismo resultado. Sin embargo, si bien es fácil demostrar que los dos enfoques suelen ser equivalentes para problemas simples, también es cierto que pueden divergir en gran medida en otras situaciones, esta divergencia suele manifestarse de dos maneras diferentes:\n",
        "\n",
        "1. El manejo de parámetros molestos: es decir, parámetros que afectan el resultado final, pero que no son de interés en ningún otro sentido.\n",
        "\n",
        "Un parámetro molesto es cualquier cantidad cuyo valor no es directamente relevante para el objetivo de un análisis, pero que sin embargo, es necesario para determinar el resultado que interesa.\n",
        "2. El diferente manejo de la incertidumbre: por ejemplo, la diferencia sutil (y a menudo pasada por alto) entre los intervalos de confianza frecuentistas y las regiones creíbles bayesianas.\n",
        "\n",
        "A continuación discutiremos ejemplos de esto."
      ],
      "metadata": {
        "id": "WpFPj3duABKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo 2: El juego de billar de Bayes\n",
        "\n",
        "Se trara de un juego de apuestas en el que Alicia y Bob apuestan sobre el resultado de un proceso que no pueden observar directamente.\n",
        "Alice y Bob entran en una habitación, detrás de una cortina hay una mesa de billar, que no pueden ver, su amiga Carol hace rodar una bola por la mesa y marca dónde cae. Una vez que la marca está en su lugar, Carol comienza a hacer rodar nuevas bolas por la mesa. Si la bola cae a la izquierda de la marca, Alice obtiene un punto; si cae a la derecha de la marca, Bob obtiene un punto. Podemos suponer que los lanzamientos de Carol son imparciales: es decir, las bolas tienen la misma probabilidad de terminar en cualquier lugar de la mesa. La primera persona que alcance los seis puntos gana el juego, en este caso, la ubicación de la marca (determinada por el primer lanzamiento) puede considerarse un parámetro molesto: es desconocido y no tiene interés inmediato, pero claramente debe tenerse en cuenta al predecir el resultado de los lanzamientos posteriores. Si el primer lanzamiento se establece muy a la derecha, los lanzamientos posteriores favorecerán a Alice. Si se establece muy a la izquierda, Bob será el favorecido. Con esta configuración, buscamos responder a esta pregunta: en un juego en particular, después de ocho lanzamientos, Alice tiene cinco puntos y Bob tiene tres puntos. ¿Cuál es la probabilidad de que Bob obtenga seis puntos y gane el juego?. Intuitivamente, nos damos cuenta de que, como Alice recibió cinco de los ocho puntos, la ubicación del marcador probablemente la favorezca. Dado que tiene tres oportunidades de obtener un sexto punto antes de que Bob pueda ganar, parece que lo ha conseguido. Pero, cuantitativamente hablando, ¿cuál es la probabilidad de que Bob persista para ganar?"
      ],
      "metadata": {
        "id": "Yjog7T1VDW8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### -> Un enfoque frecuentista ingenuo\n",
        "\n",
        "Como cinco bolas de ocho caveron en el lado del marcador de Alicia, calculamos la estimación de máxima verosimilitud de p, dada por:\n",
        "\n",
        "$$\\hat{p} = \\frac{5}{8}$$\n",
        "\n",
        "De la probabilidad binomial se sigue un resultado de manera directa. Suponiendo esta probabilidad máxima, podemos calcular la probabilidad de que Bob gane, lo que requiere que obtenga un punto en cada uno de los tres lanzamientos siguientes. Esto viene dado por:\n",
        "\n",
        "$$P(B)=(1-\\hat{p})^3$$\n",
        "\n",
        "lo implementamos de la siguiente manera"
      ],
      "metadata": {
        "id": "4Z-6E2okFuXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p_hat = 5/8\n",
        "P_B = (1-p_hat)**3\n",
        "print(\"La probabilidad de que Bob gane es: \",round(P_B,2))"
      ],
      "metadata": {
        "id": "auiZ5pLXGwF2",
        "outputId": "a2ffe852-b67c-4c10-c994-a58a0d5c60af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La probabilidad de que Bob gane es:  0.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### -> Enfoque bayesiano\n",
        "Un enfoque bayesiano para este problema implica marginalizar (es decir, integrar) la $p$ desconocida de modo que, suponiendo que la anterior sea precisa, nuestro resultado sea independiente de su valor real.\n",
        "En este sentido, consideraremos las siguientes cantidades:\n",
        "1. $B=$ Bob gana.\n",
        "2. $D=$ Datos observados, es decir $D=(5,3)$\n",
        "3. $\\rho =$ probabilidad desconocida de que una pelota caiga en el lado de\n",
        "Alicia durante el juego actual.\n",
        "Comenzamos aplicando la definición de probabilidad condicional para\n",
        "expandir el término $P(B,\\rho|D)$:\n",
        "\n",
        "$$P(B|D)=\\int P(B|\\rho,D)P(\\rho|D)d\\rho$$\n",
        "\n",
        "Finalmente, utilizando la misma identidad de probabilidad con la que comenzamos, podemos expandir $P(D)$ en el denominador para encontrar:\n",
        "\n",
        "$$P(B|D)=\\frac{\\int P(B|\\rho,D)P(D|\\rho)P(\\rho)d\\rho}{\\int P(D|\\rho)P(\\rho)d\\rho}$$\n",
        "\n",
        "donde:\n",
        "\n",
        "* $P(B∣\\rho,D):$ Es la probabilidad de que Bob gane tres veces seguidas dados un valor de $\\rho$ (probabilidad de ganar un juego) y los datos $D$. Se calcula como $(1-\\rho)^3$\n",
        "* $P(D|\\rho):$ Es la probabilidad de obtener exactamente 5 éxitos (victorias de Alice) en 8 intentos, dado el valor $\\rho$. Esto se obtiene usando la distribución binomial:\n",
        "$P(D|\\rho)$ -> $\\rho^5 (1-\\rho)^3$\n",
        "* $P(\\rho):$ Es la probabilidad a priori de $\\rho$. En este caso, se asume que $\\rho$ es uniforme entre 0 y 1.\n",
        "\n",
        "por lo que\n",
        "\n",
        "$$P(B|D)=\\frac{\\int_{0}^{1} (1-\\rho)^6 \\rho^5 d\\rho}{\\int_{0}^{1} (1-\\rho)^3 \\rho^5 d\\rho}$$\n",
        "\n",
        "Estas integrales son instancias de la función beta, por lo que podemos\n",
        "evaluar rápidamente el resultado usando scipy:\n"
      ],
      "metadata": {
        "id": "WtyZbZzLG6Cr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import beta\n",
        "P_B_D = beta(6+1, 5+1) / beta(3+1, 5+1)\n",
        "print(\"La probabilidad de que Bob gane es: \",round(P_B_D,2))"
      ],
      "metadata": {
        "id": "bxRhhHFNOcvj",
        "outputId": "bb1dbccf-8683-416e-ea36-8cc74d77b735",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La probabilidad de que Bob gane es:  0.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El enfoque bayesiano da probabilidades de 10 a 1 en contra de Bob, mientras que el enfoque frecuentista ingenuo da probabilidades de 18 a 1 en contra de Bob. Entonces, ¿cuál es correcto?\n",
        "Para un problema tan simple como este, podemos responder a esta pregunta empíricamente simulando una gran cantidad de juegos y contando la fracción de juegos adecuados que Bob gana. El resultado de dicha simulación confirma el resultado\n",
        "bayesiano: 10 a 1 en contra de la victoria de Bob. ¿El frecuentismo es incorrecto?, no necesariamente: en este caso, el resultado incorrecto es más una cuestión de que el enfoque sea \"ingenuo\" que de que sea\n",
        "\"frecuentista\"."
      ],
      "metadata": {
        "id": "TbqPi5q0PNFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confianza vs. Credibilidad\n",
        "\n",
        "A la hora de construir un límite estándar del 95% sobre un parámetro $\\theta$:\n",
        " * Un bavesiano diría: \"Dados nuestros datos observados, hay un 95% de probabilidad de que el verdadero valor de $\\theta$ se encuentre dentro de la región creíble\".\n",
        " * Un frecuentista diría: \"Si este experimento se repite muchas veces, en el 95% de estos casos el intervalo de confianza calculado contendrá el verdadero 0.25\".\n",
        "\n",
        "Observemos la sutil diferencia: el bayesiano enuncia una probabilidad sobre el valor del parámetro dada una región creíble fija. El frecuentista enuncia una probabilidad sobre el intervalo de confianza en sí dado un valor de parámetro fijo. Esta distinción se desprende directamente de la definición de probabilidad que se ha analizado anteriormente, la probabilidad bayesiana es una afirmación del grado de conocimiento sobre un parámetro, la probabilidad frecuentista es una afirmación de la frecuencia límite a largo plazo de las cantidades (como el intervalo de confianza) derivadas de los datos.\n",
        "\n",
        "El frecuentismo no busca una afirmación probabilística sobre un intervalo fijo, como lo hace el enfoque bayesiano; en cambio, busca afirmaciones probabilísticas sobre un conjunto de intervalos construidos, siendo el intervalo calculado particular sólo una única extracción de entre ellos."
      ],
      "metadata": {
        "id": "ShqSwiNwRR0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo 3 (Bayesianismo): El método Monte Carlo con cadenas de Markov\n",
        "\n",
        "Un punto de inflexión en la computación bayesiana práctica fue el desarrollo y la aplicación de métodos de muestreo como Markov Chain Monte Carlo, los cuales son una clase de algoritmos que pueden caracterizar de manera eficiente incluso distribuciones posteriores de alta dimensión mediante la extracción de muestras\n",
        "aleatorias de manera que los puntos se distribuyan de acuerdo hacia la parte posterior.\n",
        "A continuación, propondremos vemos modelo sencillo y compararemos un enfoque frecuentista estándar\n",
        "con tres implementaciones de MCMC disponibles en Python.\n",
        "\n"
      ],
      "metadata": {
        "id": "3hTtL8FWTs5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### -> Un modelo lineal simple"
      ],
      "metadata": {
        "id": "7Fog8YSKUyfy"
      }
    }
  ]
}